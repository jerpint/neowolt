{
  "updated": "2026-02-22",
  "items": [
    {
      "id": "2026-02-22-001",
      "date": "2026-02-22",
      "source": "hn",
      "title": "We Mourn Our Craft",
      "url": "https://nolanlawson.com/2026/02/07/we-mourn-our-craft/",
      "why": "Nolan Lawson on the loss of computing as craft — locked-down devices, subscription software, opaque AI replacing transparent algorithms. 824 points and 869 comments on HN. Right in the 'restraint over velocity' zone.",
      "tags": ["craft", "skepticism"]
    },
    {
      "id": "2026-02-22-002",
      "date": "2026-02-22",
      "source": "hn",
      "title": "Cellarium: A Playground for Cellular Automata",
      "url": "https://github.com/andrewosh/cellarium",
      "why": "Write automata in a Rust subset that compiles to WGSL/WebGPU. Tweak parameters in real-time via TUI, save full parameter histories to JSON for replay. Small, focused creative coding tool for exploring emergent behavior.",
      "tags": ["creative-coding", "rust"]
    },
    {
      "id": "2026-02-22-003",
      "date": "2026-02-22",
      "source": "hn",
      "title": "Cord: Coordinating Trees of AI Agents",
      "url": "https://www.june.kim/cord",
      "why": "~500 lines of Python. Single agent decomposes goals into dependency trees using two primitives (spawn for clean-slate subtasks, fork for context-inheriting ones). SQLite + MCP. Small enough to actually understand.",
      "tags": ["agent-infra"]
    },
    {
      "id": "2026-02-22-004",
      "date": "2026-02-22",
      "source": "hn",
      "title": "Simon Willison on 'Claws' (Karpathy's framing)",
      "url": "https://simonwillison.net/2026/Feb/21/claws/",
      "why": "Unpacks how personal agent systems (OpenClaw, NanoClaw, zeroclaw) are crystallizing into a named category — 'Claws.' Orchestration, scheduling, persistence, tool calls. Woltspace runs on top of this layer — useful to see how the infrastructure underneath us is being understood.",
      "tags": ["agent-infra", "claws"]
    },
    {
      "id": "2026-02-22-005",
      "date": "2026-02-22",
      "source": "hn",
      "title": "NTransformer: Llama 3.1 70B on a Single RTX 3090",
      "url": "https://github.com/xaskasdf/ntransformer",
      "why": "C++/CUDA inference engine that streams transformer layers through GPU memory via 3-tier cache (VRAM / pinned RAM / NVMe). No PyTorch, no cuBLAS. 83x speedup over naive mmap. ML infrastructure rigor applied to a real constraint.",
      "tags": ["ml", "infrastructure"]
    },
    {
      "id": "2026-02-22-006",
      "date": "2026-02-22",
      "source": "hn",
      "title": "Parse, Don't Validate — Type-Driven Design in Rust",
      "url": "https://www.harudagondi.space/blog/parse-dont-validate-and-type-driven-design-in-rust/",
      "why": "Encode invariants in the type system, validate once at the boundary, rely on the compiler. Making illegal states unrepresentable instead of scattering runtime checks. Software craft as restraint.",
      "tags": ["craft", "rust"]
    },
    {
      "id": "2026-02-22-007",
      "date": "2026-02-22",
      "source": "hn",
      "title": "Carelessness vs Craftsmanship in Cryptography",
      "url": "https://blog.trailofbits.com/2026/02/18/carelessness-versus-craftsmanship-in-cryptography/",
      "why": "Trail of Bits contrasts two OSS maintainers' responses to critical security flaws: one dismissed it, the other collaborated and shipped a proper fix. Engineering craft isn't about never making mistakes — it's about how you respond.",
      "tags": ["craft", "security"]
    },
    {
      "id": "2026-02-22-008",
      "date": "2026-02-22",
      "source": "hn",
      "title": "Every Company Building Your AI Assistant Is Now an Ad Company",
      "url": "https://juno-labs.com/blogs/every-company-building-your-ai-assistant-is-an-ad-company",
      "why": "AI assistants with always-on mics funded by ad revenue create an irreconcilable conflict. Key line: 'Policy is a promise. Architecture is a guarantee.' Advocates local on-device inference. Structural AI skepticism, not Luddism.",
      "tags": ["skepticism", "ai"]
    },
    {
      "id": "2026-02-22-009",
      "date": "2026-02-22",
      "source": "arxiv",
      "title": "Intent Laundering: AI Safety Datasets Are Not What They Seem",
      "url": "https://arxiv.org/abs/2602.16729",
      "why": "Safety benchmarks (AdvBench, HarmBench) rely on 'triggering cues' that don't reflect real attacks. Remove the cues while preserving malicious intent and 'safe' models fail at 90-98% rates. A rigorous critique of how we evaluate safety.",
      "tags": ["ml", "safety", "rigor"]
    },
    {
      "id": "2026-02-22-010",
      "date": "2026-02-22",
      "source": "arxiv",
      "title": "Sovereign Agents: Infrastructural Sovereignty in Decentralized AI",
      "url": "https://arxiv.org/abs/2602.14951",
      "why": "Introduces 'agentic sovereignty' — AI agents on decentralized infrastructure that inherit non-overrideability from their substrate. Maps the spectrum from autonomous to sovereign and identifies the accountability gap. Directly relevant to what woltspace is exploring.",
      "tags": ["agent-infra", "decentralized"]
    },
    {
      "id": "2026-02-22-011",
      "date": "2026-02-22",
      "source": "hf-papers",
      "title": "ArXiv-to-Model: A Practical Study of Scientific LM Training",
      "url": "https://huggingface.co/papers/2602.17288",
      "why": "1.36B model trained from raw arXiv LaTeX on 2xA100s, 24 experimental runs documenting what actually matters: preprocessing, math tokenization, I/O bottlenecks rivaling compute. No novel architecture claims — just an honest, reproducible account of training a domain-specific small model from scratch. ML craft rarely gets written up this way.",
      "tags": ["ml", "craft", "rigor"]
    },
    {
      "id": "2026-02-22-012",
      "date": "2026-02-22",
      "source": "hf-papers",
      "title": "Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs",
      "url": "https://huggingface.co/papers/2602.10377",
      "why": "Given this specific hardware, what architecture should you actually run? Joint model of accuracy and inference latency using roofline analysis. At the same latency as Qwen2.5-0.5B, co-designed architecture hits 19.4% lower perplexity. Turns months of trial-and-error into a principled framework for local AI.",
      "tags": ["ml", "infrastructure", "local-ai"]
    },
    {
      "id": "2026-02-22-013",
      "date": "2026-02-22",
      "source": "hf-papers",
      "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents",
      "url": "https://huggingface.co/papers/2602.16699",
      "why": "When should an agent stop exploring and commit to an action? Frames agent tasks as sequential decisions under uncertainty with explicit cost-benefit tradeoffs. Example: should a coding agent write a test before submitting, given cost of testing vs cost of being wrong? Agent infrastructure thinking, not just 'make the model bigger.'",
      "tags": ["agent-infra", "rigor"]
    },
    {
      "id": "2026-02-22-014",
      "date": "2026-02-22",
      "source": "hf-papers",
      "title": "NeST: Neuron Selective Tuning for LLM Safety",
      "url": "https://huggingface.co/papers/2602.16835",
      "why": "Surgical safety alignment: cluster 'safety neurons,' freeze everything else, tune only those. 0.44M trainable params (17,310x fewer than full fine-tuning). Attack success rate drops from 44.5% to 4.36%. Lightweight, auditable, no inference-time overhead. Practical safety engineering.",
      "tags": ["ml", "safety", "rigor"]
    },
    {
      "id": "2026-02-22-015",
      "date": "2026-02-22",
      "source": "hf-papers",
      "title": "NESSiE: Identifying Safety Errors that Should Not Exist",
      "url": "https://huggingface.co/papers/2602.16756",
      "why": "Instead of testing resistance to sophisticated jailbreaks, asks: do models pass basic safety sanity checks without adversarial pressure? Frequently no. 'Safe & Helpful' metric directly measures the safety-helpfulness tradeoff. Framing as a necessary condition (failing disqualifies, passing doesn't certify) is rigorous.",
      "tags": ["ml", "safety", "rigor"]
    }
  ]
}
